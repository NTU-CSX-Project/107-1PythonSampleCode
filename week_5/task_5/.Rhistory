library('xml2')
#Converting the description data to text
description_data_100 <- html_text(description_data_html_100)
install.packages('xml2')
install.packages('rvest')
install.packages('NLP')
install.packages('tm')
install.packages('stringr')
install.packages('wordcloud')
install.packages('RColorBrewer')
install.packages("xml2")
install.packages("tm")
knitr::opts_chunk$set(echo = TRUE)
install.packages('xml2')
install.packages('rvest')
install.packages('NLP')
install.packages('tm')
install.packages('stringr')
install.packages('wordcloud')
install.packages('RColorBrewer')
install.packages('xml2')
install.packages('rvest')
install.packages('NLP')
install.packages('tm')
install.packages('stringr')
install.packages('wordcloud')
install.packages('RColorBrewer')
library('xml2')
library('rvest')
library('NLP')
library('tm')
library('stringr')
library('RColorBrewer')
library('wordcloud')
#Specifying the url for desired website to be scrapped
url_100 <- 'http://www.imdb.com/search/title?count=100&release_date=2017,2017&title_type=feature&page=1&ref_=adv_nxt'
url_200 <- 'http://www.imdb.com/search/title?count=100&release_date=2017,2017&title_type=feature&page=2&ref_=adv_nxt'
url_300 <- 'http://www.imdb.com/search/title?count=100&release_date=2017,2017&title_type=feature&page=3&ref_=adv_nxt'
url_400 <- 'http://www.imdb.com/search/title?count=100&release_date=2017,2017&title_type=feature&page=4&ref_=adv_nxt'
url_500 <- 'http://www.imdb.com/search/title?count=100&release_date=2017,2017&title_type=feature&page=5&ref_=adv_nxt'
#Reading the HTML code from the website
webpage_100 <- read_html(url_100)
webpage_200 <- read_html(url_200)
webpage_300 <- read_html(url_300)
webpage_400 <- read_html(url_400)
webpage_500 <- read_html(url_500)
#Using CSS selectors to scrap the description section
description_data_html_100 <- html_nodes(webpage_100,'.text-muted+ .text-muted , .ratings-bar+ .text-muted')
description_data_html_200 <- html_nodes(webpage_200,'.text-muted+ .text-muted , .ratings-bar+ .text-muted')
description_data_html_300 <- html_nodes(webpage_300,'.text-muted+ .text-muted , .ratings-bar+ .text-muted')
description_data_html_400 <- html_nodes(webpage_400,'.text-muted+ .text-muted , .ratings-bar+ .text-muted')
description_data_html_500 <- html_nodes(webpage_500,'.text-muted+ .text-muted , .ratings-bar+ .text-muted')
knitr::opts_chunk$set(echo = TRUE)
library('xml2')
library('rvest')
library('NLP')
library('tm')
library('stringr')
library('RColorBrewer')
library('wordcloud')
#Converting the description data to text
description_data_100 <- html_text(description_data_html_100)
knitr::opts_chunk$set(echo = TRUE)
library('xml2')
library('rvest')
library('NLP')
library('tm')
library('stringr')
library('RColorBrewer')
library('wordcloud')
#Converting the description data to text
description_data_100 <- html_text(description_data_html_100)
knitr::opts_chunk$set(echo = TRUE)
library('xml2')
library('rvest')
library('NLP')
library('tm')
library('stringr')
library('RColorBrewer')
library('wordcloud')
#Converting the description data to text
description_data_100 <- html_text(description_data_html_100)
knitr::opts_chunk$set(echo = TRUE)
library('xml2')
library('rvest')
library('NLP')
library('tm')
library('stringr')
library('RColorBrewer')
library('wordcloud')
#Specifying the url for desired website to be scrapped
url_100 <- 'http://www.imdb.com/search/title?count=100&release_date=2017,2017&title_type=feature&page=1&ref_=adv_nxt'
url_200 <- 'http://www.imdb.com/search/title?count=100&release_date=2017,2017&title_type=feature&page=2&ref_=adv_nxt'
url_300 <- 'http://www.imdb.com/search/title?count=100&release_date=2017,2017&title_type=feature&page=3&ref_=adv_nxt'
url_400 <- 'http://www.imdb.com/search/title?count=100&release_date=2017,2017&title_type=feature&page=4&ref_=adv_nxt'
url_500 <- 'http://www.imdb.com/search/title?count=100&release_date=2017,2017&title_type=feature&page=5&ref_=adv_nxt'
#Reading the HTML code from the website
webpage_100 <- read_html(url_100)
webpage_200 <- read_html(url_200)
webpage_300 <- read_html(url_300)
webpage_400 <- read_html(url_400)
webpage_500 <- read_html(url_500)
#Using CSS selectors to scrap the description section
description_data_html_100 <- html_nodes(webpage_100,'.text-muted+ .text-muted , .ratings-bar+ .text-muted')
description_data_html_200 <- html_nodes(webpage_200,'.text-muted+ .text-muted , .ratings-bar+ .text-muted')
description_data_html_300 <- html_nodes(webpage_300,'.text-muted+ .text-muted , .ratings-bar+ .text-muted')
description_data_html_400 <- html_nodes(webpage_400,'.text-muted+ .text-muted , .ratings-bar+ .text-muted')
description_data_html_500 <- html_nodes(webpage_500,'.text-muted+ .text-muted , .ratings-bar+ .text-muted')
#Converting the description data to text
description_data_100 <- html_text(description_data_html_100)
description_data_200 <- html_text(description_data_html_200)
description_data_300 <- html_text(description_data_html_300)
description_data_400 <- html_text(description_data_html_400)
description_data_500 <- html_text(description_data_html_500)
#Combine char strings
description_data <- paste(description_data_100, description_data_200, description_data_300, description_data_400, description_data_500, sep = " ")
#Let's have a look at the description
head(description_data)
#Combine as one string
description_data <- paste(description_data, collapse = " ")
#Data-Preprocessing: removing '\n'
description_data2 <- gsub("\n","",description_data)
#Data-Preprocessing: removing non-words
description_data2 <- gsub("\\W"," ",description_data2)
#Data-Preprocessing: removing digits
description_data2 <- gsub("\\d"," ",description_data2)
#Data-Preprocessing: changing all to lower case
description_data2 <- tolower(description_data2)
#Data-Preprocessing: removing stopwords
description_data2 <- removeWords(description_data2,stopwords())
#Data-Preprocessing: removing single letters
description_data2 <- gsub("\\b[A-z]\\b{1}"," ",description_data2)
#Data-Preprocessing: removing irrelevant words
description_data2 <- gsub("see"," ",description_data2)
description_data2 <- gsub("full"," ",description_data2)
description_data2 <- gsub("summary"," ",description_data2)
#Data-Preprocessing: removing whitespaces
description_data2 <- stripWhitespace(description_data2)
#Data-Preprocessing: split up processed string into a list of separate words
textbag <- str_split(description_data2, "\\s+")
#Data-Preprocessing: unlist textbag into separate characters
textbag <- unlist(textbag)
wordcloud(textbag, min.freq = 10, random.order = FALSE, scale=c(5, 0.5), color=brewer.pal(6, "Dark2"))
head
View(head)
View(Userhead)
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
View(Userhead)
Userhead
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
res$content
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
View(getResult)
res$content
getResult = content(res$content)
getResult = res %>% content()
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
View(getResult)
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
getResult
source('C:/Users/pecu6/Desktop/house.r', encoding = 'UTF-8', echo=TRUE)
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
getResult
res = GET(url=Myurl, headers = Userhead)
getResult = content(res)
getResult
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
getResult
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
getResult
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
getResult
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
res
res$headers
res$all_headers
header(res)
res = GET(url=Myurl)
res
res$handle
res$all_headers
GET("http://httpbin.org/user-agent")
GET("https://buy.yungching.com.tw/region/%E5%8F%B0%E5%8C%97%E5%B8%82-_c/")
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
getResult
source('C:/Users/pecu6/Desktop/house.r', encoding = 'UTF-8', echo=TRUE)
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
getResult
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
getResult
View(getResult)
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
getResult
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
getResult
getResult[1]
getResult[1]$node
getResult[2]
getResult[2]$doc
getResult
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
getResult
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
getResult
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
getResult
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
getResult
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
? html
html
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
getResult
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
getResult
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
getResult
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
getResult
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
getResult
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
getResult
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
getResult
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
getResult
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
getResult
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
getResult
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
getResult
res
res$content
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
getResult
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
getResult
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
getResult
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
getResult
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
getResult
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
getResult
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
getResult
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
getResult
library(rjson)
jsonData = fromJSON(getResult)
library(XML)
doc = htmlParse(getResult)
doc
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
doc
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
doc
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
data
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
data
data[1]
print(data[1])
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
text
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
text
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
text
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
source('C:/Users/pecu6/Desktop/house.r', echo=TRUE)
View(result)
source('C:/Users/pecu6/Desktop/getTest.R', echo=TRUE)
source('C:/Users/pecu6/Desktop/getTest.R', echo=TRUE)
source('C:/Users/pecu6/Desktop/getTest.R', echo=TRUE)
post$posts
source('C:/Users/pecu6/Desktop/kmeans.R', echo=TRUE)
source('C:/Users/pecu6/Desktop/kmeans.R', echo=TRUE)
attitude
source('C:/Users/pecu6/Desktop/kmeans.R', echo=TRUE)
View(dat)
source('C:/Users/pecu6/Desktop/kmeans.R', echo=TRUE)
View(dat)
source('C:/Users/pecu6/Desktop/kmeans.R', echo=TRUE)
source('C:/Users/pecu6/Desktop/kmeans.R', echo=TRUE)
source('C:/Users/pecu6/Desktop/kmeans.R', echo=TRUE)
source('C:/Users/pecu6/Desktop/kmeans.R', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
View(x)
y
round(log(0.94))
round(log(1.94))
round(log(2.94))
round(0.94)
round(1.94)
library(NLP)
library(tm)
library(proxy)
library(dplyr)
library(NLP)
library(tm)
library(proxy)
library(stats)
library(dplyr)
datasets::cars
datasets::AirPassengers
datasets::CO2
datasets::women
datasets::BOD
datasets::iris
datasets::ChickWeight
class(datasets::ChickWeight)
a = datasets::ChickWeight
View(a)
class(a$Diet)
ChickWeight
summary(ChickWeight )
View(aids)
aids
datasets::BOD
datasets::euro
library(datasets)
aids
install.packages("MASS")
library(MASS)
aids2
Aids2
Aids2$state
Aids2$age
class(Aids2$state)
level(Aids2$state)
Level(Aids2$state)
table(Aids2$state)
class(Aids$age)
class(Aids2$age)
class(Aids2$T.categ)
class(Aids2$diag)
View(Aids2)
class(Aids2$diag)
table(Aids2$T.categ)
table(Aids2$state)
ggplot(Aids2, aes(x=age, y=diag, color=state)) +
geom_point() +
stat_smooth(method="lm")
library(ggplot2)
ggplot(Aids2, aes(x=age, y=diag, color=state)) +
geom_point() +
stat_smooth(method="lm")
ggplot(Aids2, aes(x=deaths, y=diag, color=state)) +
geom_point() +
stat_smooth(method="lm")
ggplot(Aids2, aes(x=death, y=diag, color=state)) +
geom_point() +
stat_smooth(method="lm")
library(mlbench)
library(MASS)
View(MASS::Animals)
View(MASS::anorexia)
airquality
temp = airquality
group_by( data = temp, Wind)
library(dplyr)
library(dplyr)
group_by( data = temp, Wind)
group_by(temp, Wind)
summarise(group_by(temp, Month), mean(Wind))
summarise(group_by(airquality, Month), mean(Wind))
source('~/.active-rstudio-document', echo=TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(httr)
url <- "http://ecshweb.pchome.com.tw/search/v3.3/all/results?q=sony&page=1&sort=rnk/dc"
res = GET(url)
res_json = content(res)
do.call(rbind,res_json$prods)
View(data.frame(do.call(rbind,res_json$prods)))
library(httr)
url <- "http://ecshweb.pchome.com.tw/search/v3.3/all/results?q=sony&page=1&sort=rnk/dc"
res = GET(url)
res_json = content(res)
do.call(rbind,res_json$prods)
raw = (data.frame(do.call(rbind,res_json$prods)))
library(knitr)
kable(raw)
View(raw)
source('~/.active-rstudio-document', encoding = 'UTF-8', echo=TRUE)
source('~/.active-rstudio-document', encoding = 'UTF-8', echo=TRUE)
x %% 400
x %% 4
x %% 100
((x %% 400==0)|((x %% 4==0)&(x %% 100!=0)))
source('~/.active-rstudio-document', echo=TRUE)
x = scan()
cat('閏年')
} else {
cat('平年')
}
x = scan()
x
if ((x %% 400==0)|((x %% 4==0)&(x %% 100!=0))) {
cat('閏年')
} else {
cat('平年')
}
x = scan()
cat('閏年')
} else {
cat('平年')
}
x = scan()
cat('閏年')
} else {
cat('平年')
}
x = scan()
source('~/.active-rstudio-document', encoding = 'UTF-8', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
data$doc
data[0]
data[1]
data[1]$node
info <- data %>% html_nodes("h5.prod_name")
info <- html_nodes(data, "h5.prod_name")
line <- "https://ecshweb.pchome.com.tw/search/v3.3/?q=iphone"
data <- read_html(line)
info <- data %>% html_nodes(".prod_name")
source('~/.active-rstudio-document', echo=TRUE)
info <- data %>% html_nodes("//*[@id=\"DGCD15-A9008KEXQ\"]/dd[2]/h5")
info <- data %>% html_nodes("//*[@id='DGCD15-A9008KEXQ']/dd[2]/h5")
info <- data %>% html_nodes("h5")
line <- "https://ecshweb.pchome.com.tw/search/v3.3/?q=iphone"
data <- read_html(line)
data <- content(data, "text")
line <- "https://ecshweb.pchome.com.tw/search/v3.3/?q=iphone"
data <- get(line)
data <- content(data, "text")
data <- GET(line)
data <- content(data, "text")
info <- data %>% html_nodes("h5")
links <- info %>% html_nodes("a") %>% html_nodes('href')
source('~/.active-rstudio-document', encoding = 'UTF-8', echo=TRUE)
source('~/.active-rstudio-document', encoding = 'UTF-8', echo=TRUE)
postId
content(res)
res
a = 1
mode(a)
typeof(a)
class(a)
b = as.factor(1)
b
mode(b)
typeof(b)
class(b)
c = 'o'
d = 'x'
c = as.factor(c)
mode(c)
typeof(c)
c = 'o'
mode(c)
typeof(c)
class(c)
c = as.factor(c)
mode(c)
typeof(c)
class(c)
c
class(b)
x = '1'
mode(x)
class(x)
b = 'f'
b = as.factor(b)
c = matrix(2,3)
g = data.frame(2,3,4,"c")
View(g)
g$X.c. = as.character(g$X.c.)
names(g) = c('a', 'b', 'c', 'd')
h = list(1,"b","a",2.2)
typeof(g)
mode(g)
class(g)
typeof(g$b)
setwd("C:/Users/pecu6/Desktop/106-2RSampleCode/week_5/task_5")
source('C:/Users/pecu6/Desktop/106-2RSampleCode/week_5/task_5/TFIDF.R', echo=TRUE)
View(keys)
View(tfidfnn)
